#!/usr/bin/env python3
import sys
import os
import asyncio
import time
import json
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

""""""
Performance Benchmarking Suite for Honeypot Detector API
""""""

import asyncio
import csv
import os
import statistics
import sys
import time
from datetime import datetime
from pathlib import Path

import httpx

from config.settings import settings

# Add project paths
project_root = Path(__file__).parent
while project_root.name != "Scorpius-main" and project_root != project_root.parent:
    project_root = project_root.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "backend"))
sys.path.insert(0, str(project_root / "packages" / "core"))

# Create mock classes for commonly missing modules


class MockSimilarityEngine:
    def __init__(self, *args, **kwargs):
        pass

    async def compare_bytecodes(self, *args, **kwargs):
        class Result:
            similarity_score = 0.85
            confidence = 0.9
            processing_time = 0.01

        return Result()

    async def cleanup(self):
        pass


class MockBytecodeNormalizer:
    async def normalize(self, bytecode):
        return bytecode.replace("0x", "").lower() if bytecode else ""


class MockMultiDimensionalComparison:
    def __init__(self, *args, **kwargs):
        pass

    async def compute_similarity(self, b1, b2):
        return {"final_score": 0.85, "confidence": 0.9, "dimension_scores": {}}


class MockTestClient:
    def __init__(self, app):
        self.app = app

    def get(self, url):
        class Response:
            status_code = 200

            def json(self):
                return {"status": "ok"}

        return Response()


# Add mocks to globals for import fallbacks
globals().update(
    {
        "SimilarityEngine": MockSimilarityEngine,
    print(f"Error: {e}")
        "MultiDimensionalComparison": MockMultiDimensionalComparison,
        "TestClient": MockTestClient,
    print(f"Error: {e}")
# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class PerformanceBenchmark:
    """Performance benchmarking for the API""""""

    def __init__(self):
        self.base_url = "http://localhost:8000"
        self.api_key = settings.API_KEY
        self.headers = {"X-API-Key": self.api_key}
        self.results = []

    async def benchmark_endpoint(
        self,
    print(f"Error: {e}")
        method: str = "GET",
        json_data: dict = None,
    print(f"Error: {e}")
    ):
        """Benchmark a specific endpoint""""""
        print(f"[CHART] Benchmarking {method} {endpoint} ({iterations} iterations)...")

        response_times = []
        status_codes = []
        errors = 0

        async with httpx.AsyncClient(base_url=self.base_url, timeout=30.0) as client:
            for i in range(iterations):
                start_time = time.perf_counter()

                try:
                    if method == "GET":
                        response = await client.get(endpoint, headers=self.headers)
                    elif method == "POST":
                        response = await client.post(
                            endpoint, json=json_data, headers=self.headers
                        
                    else:
                        raise ValueError(f"Unsupported method: {method}")

                    end_time = time.perf_counter()
                    duration = end_time - start_time

                    response_times.append(duration)
                    status_codes.append(response.status_code)

                    if i % 20 == 0:
                        print(
                            f"  Progress: {i}/{iterations} ({i / iterations * 100:.1f}%)"
                        
                except Exception as e:
                    errors += 1
                    print(f"  Error on iteration {i}: {str(e)}")

                # Small delay to prevent overwhelming the server
                await asyncio.sleep(0.01)

        # Calculate statistics
        if response_times:
            stats = {
                "endpoint": endpoint,
    print(f"Error: {e}")
                "iterations": iterations,
                "errors": errors,
    print(f"Error: {e}")
                "min_time": min(response_times),
                "max_time": max(response_times),
    print(f"Error: {e}")
                "median_time": statistics.median(response_times),
                "p95_time": self.percentile(response_times, 95),
    print(f"Error: {e}")
                "requests_per_second": (
                    1 / statistics.mean(response_times) if response_times else 0
                ),
    print(f"Error: {e}")
                    zip(
                        *zip(
                            *[
                                (code, status_codes.count(code))
                                for code in set(status_codes)
                            ]
                        
                    
                ),
    print(f"Error: {e}")
            self.results.append(stats)
            self.print_stats(stats)

            return stats
        else:
            print("[FAIL] No successful requests!")
            return None

    def percentile(self, data, percentile):
        """Calculate percentile""""""
        if not data:
            return 0
        sorted_data = sorted(data)
        index = (percentile / 100) * (len(sorted_data) - 1)
        lower = int(index)
        upper = min(lower + 1, len(sorted_data) - 1)
        weight = index - lower
        return sorted_data[lower] * (1 - weight) + sorted_data[upper] * weight

    def print_stats(self, stats):
        """Print formatted statistics""""""
        print(f"  [PASS] Results for {stats['method']} {stats['endpoint']}:")
        print(f"     Success Rate: {stats['success_rate']:.1f}%")
        print(f"     Average Time: {stats['avg_time'] * 1000:.2f}ms")
        print(f"     Median Time: {stats['median_time'] * 1000:.2f}ms")
        print(f"     95th Percentile: {stats['p95_time'] * 1000:.2f}ms")
        print(f"     99th Percentile: {stats['p99_time'] * 1000:.2f}ms")
        print(f"     Requests/Second: {stats['requests_per_second']:.2f}")
        print(f"     Status Codes: {stats['status_codes']}")
        print()

    async def run_comprehensive_benchmark(self):
        """Run comprehensive benchmarks on all endpoints""""""
        print(">> Running Comprehensive Performance Benchmark")
        print("=" * 60)

        # Health endpoints
        await self.benchmark_endpoint("/health", iterations=200)
        await self.benchmark_endpoint("/health/status", iterations=100)

        # Dashboard endpoints
        await self.benchmark_endpoint("/api/v1/dashboard/stats", iterations=50)
        await self.benchmark_endpoint("/api/v1/dashboard/trends?days=7", iterations=50)
        await self.benchmark_endpoint(
            "/api/v1/dashboard/search?limit=10", iterations=50
        
        # Analysis endpoint (might fail if services not configured)
        analysis_data = {
            "address": "0x1234567890abcdef1234567890abcdef12345678",
    print(f"Error: {e}")
            "deep_analysis": False,
        }
        await self.benchmark_endpoint(
            "/api/v1/analyze", method="POST", json_data=analysis_data, iterations=10
        
        print("=" * 60)
        print("[CHART] Benchmark Summary")
        print("=" * 60)

        if self.results:
            # Sort by average response time
            sorted_results = sorted(self.results, key=lambda x: x["avg_time"])

            print("üèÜ Fastest to Slowest Endpoints:")
            for i, result in enumerate(sorted_results, 1):
                print(
                    f"{i}. {result['method']} {result['endpoint']}: "
                    f"{result['avg_time'] * 1000:.2f}ms avg, "
                    f"{result['success_rate']:.1f}% success"
                
            # Generate CSV report
            self.generate_csv_report()

        return self.results

    async def stress_test(self, concurrent_users: int = 10, duration_seconds: int = 30):
        """Stress test with concurrent users""""""
        print(
            f"[FIRE] Stress Test: {concurrent_users} concurrent users for {duration_seconds}s"
        
        print("=" * 60)

        async def user_simulation(user_id: int):
            """Simulate a user making requests""""""
            requests_made = 0
            errors = 0
            start_time = time.time()

            async with httpx.AsyncClient(
                base_url=self.base_url, timeout=10.0
            ) as client:
                while time.time() - start_time < duration_seconds:
                    try:
                        # Simulate typical user behavior
                        endpoints = [
                            "/health",
    print(f"Error: {e}")
                            "/api/v1/dashboard/trends?days=7",
                        ]

                        endpoint = endpoints[requests_made % len(endpoints)]
                        response = await client.get(endpoint, headers=self.headers)

                        if response.status_code == 200:
                            requests_made += 1
                        else:
                            errors += 1

                        # Simulate user think time
                        await asyncio.sleep(0.5)

                    except Exception:
                        errors += 1

            return {
                "user_id": user_id,
    print(f"Error: {e}")
                "errors": errors,
                "duration": time.time() - start_time,
    print(f"Error: {e}")
        # Run concurrent users
        tasks = [user_simulation(i) for i in range(concurrent_users)]
        user_results = await asyncio.gather(*tasks)

        # Calculate aggregate stats
        total_requests = sum(r["requests"] for r in user_results)
        total_errors = sum(r["errors"] for r in user_results)
        avg_requests_per_user = total_requests / concurrent_users
        error_rate = (
            (total_errors / (total_requests + total_errors)) * 100
            if (total_requests + total_errors) > 0
            else 0
        
        print(f"[CHART] Stress Test Results:")
        print(f"   Total Requests: {total_requests}")
        print(f"   Total Errors: {total_errors}")
        print(f"   Error Rate: {error_rate:.2f}%")
        print(f"   Avg Requests/User: {avg_requests_per_user:.1f}")
        print(f"   Requests/Second: {total_requests / duration_seconds:.2f}")

        return user_results

    def generate_csv_report(self):
        """Generate CSV report of benchmark results""""""
        filename = f"benchmark_report_{
            datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

        with open(filename, "w", newline="") as csvfile:
            fieldnames = [
                "endpoint",
    print(f"Error: {e}")
                "iterations",
                "success_rate",
    print(f"Error: {e}")
                "median_time_ms",
                "p95_time_ms",
    print(f"Error: {e}")
                "requests_per_second",
                "errors",
    print(f"Error: {e}")
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()

            for result in self.results:
                row = {
                    "endpoint": result["endpoint"],
    print(f"Error: {e}")
                    "iterations": result["iterations"],
                    "success_rate": f"{
                        result['success_rate']:.2f}",
    print(f"Error: {e}")
                        result['avg_time'] * 1000:.2f}",
                    "median_time_ms": f"{
                        result['median_time'] * 1000:.2f}",
    print(f"Error: {e}")
                        result['p95_time'] * 1000:.2f}",
                    "p99_time_ms": f"{
                        result['p99_time'] * 1000:.2f}",
    print(f"Error: {e}")
                        result['requests_per_second']:.2f}",
                    "errors": result["errors"],
    print(f"Error: {e}")
                writer.writerow(row)

        print(f"[CHART] Benchmark report saved to: {filename}")


async def main():
    """Main benchmarking function""""""
    benchmark = PerformanceBenchmark()

    print(">> Honeypot Detector - Performance Benchmark Suite")
    print("Make sure the API is running on http://localhost:8000")
    print("=" * 60)

    # Run comprehensive benchmark
    await benchmark.run_comprehensive_benchmark()

    # Run stress test
    await benchmark.stress_test(concurrent_users=5, duration_seconds=15)

    print("\n[CELEBRATION] Performance benchmarking completed!")
    print("Check the generated CSV file for detailed results.")


if __name__ == "__main__":
    asyncio.run(main())

if __name__ == '__main__':
    print('Running test file...')
    
    # Run all test functions
    test_functions = [name for name in globals() if name.startswith('test_')]
    
    for test_name in test_functions:
        try:
            test_func = globals()[test_name]
            if asyncio.iscoroutinefunction(test_func):
                asyncio.run(test_func())
            else:
                test_func()
            print(f'‚úì {test_name} passed')
        except Exception as e:
            print(f'‚úó {test_name} failed: {e}')
    
    print('Test execution completed.')