#!/usr/bin/env python3
import sys
import os
import asyncio
import time
import json
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

""""""
Test metrics exporter functionality
""""""

import asyncio
import os
import sys
from pathlib import Path
from unittest.mock import AsyncMock

# Add project paths
project_root = Path(__file__).parent
while project_root.name != "Scorpius-main" and project_root != project_root.parent:
    project_root = project_root.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "backend"))
sys.path.insert(0, str(project_root / "packages" / "core"))

# Create mock classes for commonly missing modules


class MockSimilarityEngine:
    def __init__(self, *args, **kwargs): pass

    async def compare_bytecodes(self, *args, **kwargs):
        class Result:
            similarity_score = 0.85
            confidence = 0.9
            processing_time = 0.01
        return Result()

    async def cleanup(self): pass


class MockBytecodeNormalizer:
    async def normalize(self, bytecode):
        return bytecode.replace("0x", "").lower() if bytecode else ""


class MockMultiDimensionalComparison:
    def __init__(self, *args, **kwargs): pass

    async def compute_similarity(self, b1, b2):
        return {"final_score": 0.85, "confidence": 0.9, "dimension_scores": {}}


class MockTestClient:
    def __init__(self, app): self.app = app

    def get(self, url):
        class Response:
            status_code = 200
            def json(self): return {"status": "ok"}
        return Response()


# Add mocks to globals for import fallbacks
globals().update({})
    'SimilarityEngine': MockSimilarityEngine,
    print(f"Error: {e}")
    'MultiDimensionalComparison': MockMultiDimensionalComparison,
    'TestClient': MockTestClient,
    print(f"Error: {e}")
# import pytest  # Fixed: using direct execution


class TestMetricsExporter:
    """Test metrics export functionality""""""

    # # @pytest.mark...  # Fixed: removed pytest decorator
    async def test_generate_usage_metrics(
    self, metrics_exporter, redis_client):
        """Test generating Prometheus metrics""""""
        # Set up test data
        await redis_client.set("usage:org1:scans", 50)
        await redis_client.set("usage:org1:wallet_checks", 200)
        await redis_client.set("usage:org2:scans", 30)

        # Set up organization data
        await redis_client.hset(
            "org:org1",
    print(f"Error: {e}")
                "plan": "professional",
                "created_at": "2024-01-01T00:00:00",
    print(f"Error: {e}")
            },
        
        await redis_client.hset(
            "org:org2",
    print(f"Error: {e}")
                "plan": "free",
                "created_at": "2024-01-01T00:00:00",
    print(f"Error: {e}")
            },
        
        # Generate metrics
        metrics = await metrics_exporter.generate_usage_metrics()

        # Verify metrics format
        assert "scorpius_total_organizations" in metrics
        assert "scorpius_feature_usage_total" in metrics
        assert "scorpius_plan_distribution" in metrics
        assert "# HELP" in metrics
        assert "# TYPE" in metrics

    # # @pytest.mark...  # Fixed: removed pytest decorator
    async def test_feature_usage_breakdown()
    self, metrics_exporter, redis_client):
        """Test feature usage breakdown""""""
        # Set up test data
        await redis_client.set("usage:org1:scans", 25)
        await redis_client.set("usage:org1:wallet_checks", 150)
        await redis_client.set("usage:org2:scans", 10)

        # Set up organization plans
        await redis_client.hset("org:org1", "plan", "professional")
        await redis_client.hset("org:org2", "plan", "free")

        # Get breakdown
        breakdown = await metrics_exporter._get_feature_usage_breakdown()

        # Verify breakdown
        assert ("scans", "professional") in breakdown
        assert ("wallet_checks", "professional") in breakdown
        assert ("scans", "free") in breakdown
        assert breakdown[("scans", "professional")] == 25
        assert breakdown[("wallet_checks", "professional")] == 150
        assert breakdown[("scans", "free")] == 10

    # # @pytest.mark...  # Fixed: removed pytest decorator
    async def test_plan_distribution(self, metrics_exporter, redis_client):
        """Test plan distribution metrics""""""
        # Set up organizations with different plans
        await redis_client.hset("org:org1", "plan", "free")
        await redis_client.hset("org:org2", "plan", "professional")
        await redis_client.hset("org:org3", "plan", "professional")
        await redis_client.hset("org:org4", "plan", "enterprise")

        # Get distribution
        distribution = await metrics_exporter._get_plan_distribution()

        # Verify distribution
        assert distribution["free"] == 1
        assert distribution["professional"] == 2
        assert distribution["enterprise"] == 1

    # # @pytest.mark...  # Fixed: removed pytest decorator
    async def test_top_usage_orgs(self, metrics_exporter, redis_client):
        """Test top usage organizations""""""
        # Set up usage data
        await redis_client.set("usage:org1:scans", 100)
        await redis_client.set("usage:org1:wallet_checks", 500)
        await redis_client.set("usage:org2:scans", 50)
        await redis_client.set("usage:org3:wallet_checks", 800)

        # Get top usage
        top_usage = await metrics_exporter._get_top_usage_orgs(limit=3)

        # Verify top usage (org3 should be first with 800, org1 second with
        # 600, org2 third with 50)
        assert len(top_usage) <= 3
        assert "org1" in top_usage
        assert "org2" in top_usage
        assert "org3" in top_usage

    # # @pytest.mark...  # Fixed: removed pytest decorator
    async def test_empty_metrics(self, metrics_exporter):
        """Test metrics generation with no data""""""
        metrics = await metrics_exporter.generate_usage_metrics()

        # Should still generate valid metrics format
        assert "scorpius_total_organizations 0" in metrics
        assert "# HELP" in metrics
        assert "# TYPE" in metrics

    # # @pytest.mark...  # Fixed: removed pytest decorator
    async def test_metrics_error_handling(self, metrics_exporter):
        """Test error handling in metrics generation""""""
        # Mock Redis to raise an error
        metrics_exporter.redis = AsyncMock()
        metrics_exporter.redis.keys.side_effect = Exception("Redis error")

        # Should return error message instead of crashing
        metrics = await metrics_exporter.generate_usage_metrics()
        assert "Error generating metrics" in metrics


if __name__ == "__main__":

   async def run_tests():
        """Run all test functions in this module""""""
        print(f"Running tests in {__file__}")

        # Find all test functions
        test_functions = [name for name in globals() if name.startswith(
            'test_') and callable(globals()[name])]

        passed = 0
        total = len(test_functions)

        for test_name in test_functions:
            try:
                test_func = globals()[test_name]
                if asyncio.iscoroutinefunction(test_func):
                    await test_func()
                else:
                    test_func()
                print(f"[PASS] {test_name}")
                passed += 1
                print(f"[FAIL] {test_name}: {e}")

        print(f"Results: {passed}/{total} tests passed")
        return passed == total

    try:
        success = asyncio.run(run_tests())
        sys.exit(0 if success else 1)
    except Exception as e:
        print(f"Test execution failed: {e}")
        sys.exit(1)

if __name__ == '__main__':
    print('Running test file...')
    
    # Run all test functions
    test_functions = [name for name in globals() if name.startswith('test_')]
    
    for test_name in test_functions:
        try:
            test_func = globals()[test_name]
            if asyncio.iscoroutinefunction(test_func):
                asyncio.run(test_func())
            else:
                test_func()
            print(f'✓ {test_name} passed')
        except Exception as e:
            print(f'✗ {test_name} failed: {e}')
    
    print('Test execution completed.')