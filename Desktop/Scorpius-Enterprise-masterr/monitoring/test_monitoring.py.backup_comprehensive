#!/usr/bin/env python3
import sys
import os
import asyncio
import time
import json
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import json
import sys
from datetime import datetime
from pathlib import Path

import yaml

# Add project paths
project_root = Path(__file__).parent
while project_root.name != "Scorpius-main" and project_root != project_root.parent:
    project_root = project_root.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "backend"))
sys.path.insert(0, str(project_root / "packages" / "core"))

# Create mock classes for commonly missing modules


class MockSimilarityEngine:
    def __init__(self, *args, **kwargs):
        pass

    async def compare_bytecodes(self, *args, **kwargs):
        class Result:
            similarity_score = 0.85
            confidence = 0.9
            processing_time = 0.01

        return Result()

    async def cleanup(self):
        pass


class MockBytecodeNormalizer:
    async def normalize(self, bytecode):
        return bytecode.replace("0x", "").lower() if bytecode else ""


class MockMultiDimensionalComparison:
    def __init__(self, *args, **kwargs):
        pass

    async def compute_similarity(self, b1, b2):
        return {"final_score": 0.85, "confidence": 0.9, "dimension_scores": {}}


class MockTestClient:
    def __init__(self, app):
        self.app = app

    def get(self, url):
        class Response:
            status_code = 200

            def json(self):
                return {"status": "ok"}

        return Response()


# Add mocks to globals for import fallbacks
globals().update(
    {
        "SimilarityEngine": MockSimilarityEngine,
    print(f"Error: {e}")
        "MultiDimensionalComparison": MockMultiDimensionalComparison,
        "TestClient": MockTestClient,
    print(f"Error: {e}")
#!/usr/bin/env python3
"""""""""
# Test script for SLA monitoring and alerting system
""""""


def test_prometheus_config():
    """Test Prometheus configuration validity""""""
    print(">> Testing Prometheus configuration...")

    try:
        with open("prometheus.yml", "r") as f:
            config = yaml.safe_load(f)

        required_sections = ["global", "scrape_configs", "rule_files", "alerting"]
        missing_sections = []

        for section in required_sections:
            if section not in config:
                missing_sections.append(section)

        if missing_sections:
            print(f"[FAIL] Missing sections in prometheus.yml: {missing_sections}")
            return False

        # Check scrape configs
        scrape_configs = config.get("scrape_configs", [])
        expected_jobs = [
            "prometheus",
    print(f"Error: {e}")
            "wallet-guard",
            "usage-metering",
    print(f"Error: {e}")
            "bridge-service",
            "node-exporter",
    print(f"Error: {e}")
        configured_jobs = [job.get("job_name") for job in scrape_configs]
        missing_jobs = [job for job in expected_jobs if job not in configured_jobs]

        if missing_jobs:
            print(f"[WARNING]  Missing scrape jobs: {missing_jobs}")

        print("[PASS] Prometheus configuration valid")
        return True

    except Exception as e:
        print(f"[FAIL] Error reading prometheus.yml: {e}")
        return False


def test_alerting_rules():
    """Test alerting rules configuration""""""
    print("\n[ALERT] Testing alerting rules...")

    rule_files = ["rules/sla_alerts.yml", "rules/audit_security_alerts.yml"]

    all_valid = True
    total_rules = 0

    for rule_file in rule_files:
        try:
            with open(rule_file, "r") as f:
                rules = yaml.safe_load(f)

            groups = rules.get("groups", [])
            for group in groups:
                group_rules = group.get("rules", [])
                total_rules += len(group_rules)

                for rule in group_rules:
                    # Check required fields for alerts
                    if "alert" in rule:
                        required_fields = ["expr", "labels", "annotations"]
                        for field in required_fields:
                            if field not in rule:
                                print(
                                    f"[FAIL] Alert {
                                        rule.get(
                                            'alert',
    print(f"Error: {e}")
                                all_valid = False

            print(
                f"[PASS] {rule_file}: {len([r for g in groups for r in g.get('rules', [])])} rules"
            
        except Exception as e:
            print(f"[FAIL] Error reading {rule_file}: {e}")
            all_valid = False

    print(f"[CHART] Total alerting rules: {total_rules}")
    return all_valid


def test_alertmanager_config():
    """Test Alertmanager configuration""""""
    print("\nðŸ“§ Testing Alertmanager configuration...")

    try:
        with open("alertmanager.yml", "r") as f:
            config = yaml.safe_load(f)

        required_sections = ["global", "route", "receivers"]
        missing_sections = []

        for section in required_sections:
            if section not in config:
                missing_sections.append(section)

        if missing_sections:
            print(f"[FAIL] Missing sections in alertmanager.yml: {missing_sections}")
            return False

        # Check receivers
        receivers = config.get("receivers", [])
        expected_receivers = [
            "critical-alerts",
    print(f"Error: {e}")
            "infrastructure-team",
            "platform-team",
    print(f"Error: {e}")
            "billing-team",
        ]

        configured_receivers = [r.get("name") for r in receivers]
        missing_receivers = [
            r for r in expected_receivers if r not in configured_receivers
        ]

        if missing_receivers:
            print(f"[WARNING]  Missing receivers: {missing_receivers}")

        print(
            f"[PASS] Alertmanager configuration valid ({
                len(receivers)} receivers)"
        
        return True

    except Exception as e:
        print(f"[FAIL] Error reading alertmanager.yml: {e}")
        return False


def test_grafana_dashboards():
    """Test Grafana dashboard configurations""""""
    print("\n[CHART] Testing Grafana dashboards...")

    dashboard_files = ["dashboards/sla-dashboard.json"]

    all_valid = True

    for dashboard_file in dashboard_files:
        try:
            with open(dashboard_file, "r") as f:
                dashboard = json.load(f)

            if "dashboard" not in dashboard:
                print(f"[FAIL] {dashboard_file}: Invalid dashboard structure")
                all_valid = False
                continue

            panels = dashboard["dashboard"].get("panels", [])
            panel_count = len(panels)

            # Check for essential panels
            panel_titles = [p.get("title", "") for p in panels]
            essential_panels = [
                "Platform Availability",
    print(f"Error: {e}")
                "Error Rate",
                "Service Status",
    print(f"Error: {e}")
            ]

            missing_panels = []
            for panel in essential_panels:
                if not any(panel in title for title in panel_titles):
                    missing_panels.append(panel)

            if missing_panels:
                print(f"[WARNING]  {dashboard_file}: Missing panels: {missing_panels}")

            print(f"[PASS] {dashboard_file}: {panel_count} panels")

        except Exception as e:
            print(f"[FAIL] Error reading {dashboard_file}: {e}")
            all_valid = False

    return all_valid


def test_sla_thresholds():
    """Test SLA threshold definitions""""""
    print("\nðŸ“ Testing SLA thresholds...")

    sla_requirements = {
        "api_gateway_latency_p95": 2.5,  # seconds
        "platform_availability": 99.9,  # percentage
        "error_rate": 2.0,  # percentage
        "wallet_guard_latency_p95": 1.8,  # seconds
    }

    try:
        with open("rules/sla_alerts.yml", "r") as f:
            rules = yaml.safe_load(f)

        found_thresholds = {}

        for group in rules.get("groups", []):
            for rule in group.get("rules", []):
                if "alert" in rule:
                    alert_name = rule["alert"]
                    expr = rule["expr"]

                    # Extract thresholds from expressions
                    if "APIGatewayHighLatency" in alert_name and ">" in expr:
                        threshold = float(expr.split(">")[-1].strip())
                        found_thresholds["api_gateway_latency_p95"] = threshold

                    elif "APIGatewayHighErrorRate" in alert_name and ">" in expr:
                        threshold = float(expr.split(">")[-1].strip())
                        # Convert to percentage
                        found_thresholds["error_rate"] = threshold * 100

                    elif "WalletGuardHighLatency" in alert_name and ">" in expr:
                        threshold = float(expr.split(">")[-1].strip())
                        found_thresholds["wallet_guard_latency_p95"] = threshold

        # Validate thresholds
        all_correct = True
        for metric, expected_threshold in sla_requirements.items():
            if metric in found_thresholds:
                actual_threshold = found_thresholds[metric]
                if actual_threshold != expected_threshold:
                    print(
                        f"[WARNING]  {metric}: Expected {expected_threshold}, found {actual_threshold}"
                    
                    all_correct = False
                else:
                    print(f"[PASS] {metric}: {actual_threshold}")
            else:
                print(f"[FAIL] {metric}: Threshold not found")
                all_correct = False

        return all_correct

    except Exception as e:
        print(f"[FAIL] Error checking SLA thresholds: {e}")
        return False


def test_metrics_endpoints():
    """Test that metrics endpoints are properly configured""""""
    print("\nðŸ“ˆ Testing metrics endpoint configurations...")

    metrics_endpoints = [
        ("API Gateway", "api-gateway:8000/metrics"),
    print(f"Error: {e}")
        ("Wallet Guard", "wallet-guard:8006/metrics"),
        ("Prometheus", "localhost:9090/metrics"),
    print(f"Error: {e}")
    configured_endpoints = []
    try:
        with open("prometheus.yml", "r") as f:
            config = yaml.safe_load(f)

        for job in config.get("scrape_configs", []):
            targets = job.get("static_configs", [{}])[0].get("targets", [])
            metrics_path = job.get("metrics_path", "/metrics")

            for target in targets:
                endpoint = f"{target}{metrics_path}"
                configured_endpoints.append((job.get("job_name"), endpoint))

    except Exception as e:
        print(f"[FAIL] Error reading prometheus config: {e}")
        return False

    print(
        f"[PASS] Found {
            len(configured_endpoints)} configured metrics endpoints"
    
    return True


def generate_monitoring_report():
    """Generate comprehensive monitoring report""""""
    print("\n" + "=" * 60)
    print("[REPORT] SCORPIUS SLA MONITORING VALIDATION REPORT")
    print("=" * 60)

    test_results = {
        "prometheus_config": test_prometheus_config(),
    print(f"Error: {e}")
        "alertmanager_config": test_alertmanager_config(),
        "grafana_dashboards": test_grafana_dashboards(),
    print(f"Error: {e}")
        "metrics_endpoints": test_metrics_endpoints(),
    }

    passed_tests = sum(test_results.values())
    total_tests = len(test_results)

    print(f"\n[CHART] Test Results: {passed_tests}/{total_tests} passed")

    if passed_tests == total_tests:
        print("\n[CELEBRATION] SLA MONITORING SYSTEM VALIDATION PASSED!")
        print("\nMonitoring system includes:")
        print("[PASS] Comprehensive Prometheus configuration")
        print("[PASS] SLA alerting rules with proper thresholds")
        print("[PASS] Multi-channel alerting via Alertmanager")
        print("[PASS] Grafana dashboards for SLA visualization")
        print("[PASS] Audit trail and security monitoring")
        print("[PASS] Business metrics and compliance rules")
        print("[PASS] Infrastructure and performance monitoring")

        print("\nðŸŽ¯ SLA Coverage:")
        print("â€¢ API Gateway: <2.5s p95 latency, <2% error rate, 99.9% availability")
        print("â€¢ Wallet Guard: <1.8s p95 latency for 25 addresses")
        print("â€¢ Usage Metering: <1s API latency")
        print("â€¢ Audit Trail: Tamper detection, compliance monitoring")
        print("â€¢ Security: Failed auth, suspicious activity, rate limiting")

        return True
    else:
        print("\n[FAIL] SLA MONITORING SYSTEM VALIDATION FAILED!")
        print("Please fix the failing components above.")

        failed_tests = [test for test, result in test_results.items() if not result]
        print(f"\nFailed tests: {', '.join(failed_tests)}")

        return False


def main():
    """Main execution function""""""
    print(">> Validating Scorpius SLA Monitoring & Alerting System...")
    print("Starting validation at:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))

    success = generate_monitoring_report()

    if success:
        print("\n>> Ready for production deployment!")
        print("Next steps:")
        print("1. Configure environment variables (Slack webhooks, SMTP)")
        print("2. Deploy monitoring stack with docker-compose")
        print("3. Import Grafana dashboards")
        print("4. Test alert routing and notifications")
        return 0
    else:
        return 1


if __name__ == "__main__":
    sys.exit(main())

if __name__ == '__main__':
    print('Running test file...')
    
    # Run all test functions
    test_functions = [name for name in globals() if name.startswith('test_')]
    
    for test_name in test_functions:
        try:
            test_func = globals()[test_name]
            if asyncio.iscoroutinefunction(test_func):
                asyncio.run(test_func())
            else:
                test_func()
            print(f'âœ“ {test_name} passed')
        except Exception as e:
            print(f'âœ— {test_name} failed: {e}')
    
    print('Test execution completed.')