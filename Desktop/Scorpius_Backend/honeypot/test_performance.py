"""
Performance Benchmarking Suite for Honeypot Detector API
"""
import asyncio
import httpx
import time
import statistics
import csv
from datetime import datetime
import sys
import os

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.settings import settings

class PerformanceBenchmark:
    """Performance benchmarking for the API"""
    
    def __init__(self):
        self.base_url = "http://localhost:8000"
        self.api_key = settings.API_KEY
        self.headers = {"X-API-Key": self.api_key}
        self.results = []
    
    async def benchmark_endpoint(self, endpoint: str, method: str = "GET", 
                                json_data: dict = None, iterations: int = 100):
        """Benchmark a specific endpoint"""
        print(f"üìä Benchmarking {method} {endpoint} ({iterations} iterations)...")
        
        response_times = []
        status_codes = []
        errors = 0
        
        async with httpx.AsyncClient(base_url=self.base_url, timeout=30.0) as client:
            for i in range(iterations):
                start_time = time.perf_counter()
                
                try:
                    if method == "GET":
                        response = await client.get(endpoint, headers=self.headers)
                    elif method == "POST":
                        response = await client.post(endpoint, json=json_data, headers=self.headers)
                    else:
                        raise ValueError(f"Unsupported method: {method}")
                    
                    end_time = time.perf_counter()
                    duration = end_time - start_time
                    
                    response_times.append(duration)
                    status_codes.append(response.status_code)
                    
                    if i % 20 == 0:
                        print(f"  Progress: {i}/{iterations} ({i/iterations*100:.1f}%)")
                
                except Exception as e:
                    errors += 1
                    print(f"  Error on iteration {i}: {str(e)}")
                
                # Small delay to prevent overwhelming the server
                await asyncio.sleep(0.01)
        
        # Calculate statistics
        if response_times:
            stats = {
                "endpoint": endpoint,
                "method": method,
                "iterations": iterations,
                "errors": errors,
                "success_rate": ((iterations - errors) / iterations) * 100,
                "min_time": min(response_times),
                "max_time": max(response_times),
                "avg_time": statistics.mean(response_times),
                "median_time": statistics.median(response_times),
                "p95_time": self.percentile(response_times, 95),
                "p99_time": self.percentile(response_times, 99),
                "requests_per_second": 1 / statistics.mean(response_times) if response_times else 0,
                "status_codes": dict(zip(*zip(*[(code, status_codes.count(code)) for code in set(status_codes)])))
            }
            
            self.results.append(stats)
            self.print_stats(stats)
            
            return stats
        else:
            print("‚ùå No successful requests!")
            return None
    
    def percentile(self, data, percentile):
        """Calculate percentile"""
        if not data:
            return 0
        sorted_data = sorted(data)
        index = (percentile / 100) * (len(sorted_data) - 1)
        lower = int(index)
        upper = min(lower + 1, len(sorted_data) - 1)
        weight = index - lower
        return sorted_data[lower] * (1 - weight) + sorted_data[upper] * weight
    
    def print_stats(self, stats):
        """Print formatted statistics"""
        print(f"  ‚úÖ Results for {stats['method']} {stats['endpoint']}:")
        print(f"     Success Rate: {stats['success_rate']:.1f}%")
        print(f"     Average Time: {stats['avg_time']*1000:.2f}ms")
        print(f"     Median Time: {stats['median_time']*1000:.2f}ms")
        print(f"     95th Percentile: {stats['p95_time']*1000:.2f}ms")
        print(f"     99th Percentile: {stats['p99_time']*1000:.2f}ms")
        print(f"     Requests/Second: {stats['requests_per_second']:.2f}")
        print(f"     Status Codes: {stats['status_codes']}")
        print()
    
    async def run_comprehensive_benchmark(self):
        """Run comprehensive benchmarks on all endpoints"""
        print("üöÄ Running Comprehensive Performance Benchmark")
        print("=" * 60)
        
        # Health endpoints
        await self.benchmark_endpoint("/health", iterations=200)
        await self.benchmark_endpoint("/health/status", iterations=100)
        
        # Dashboard endpoints
        await self.benchmark_endpoint("/api/v1/dashboard/stats", iterations=50)
        await self.benchmark_endpoint("/api/v1/dashboard/trends?days=7", iterations=50)
        await self.benchmark_endpoint("/api/v1/dashboard/search?limit=10", iterations=50)
        
        # Analysis endpoint (might fail if services not configured)
        analysis_data = {
            "address": "0x1234567890abcdef1234567890abcdef12345678",
            "chain_id": 1,
            "deep_analysis": False
        }
        await self.benchmark_endpoint("/api/v1/analyze", method="POST", 
                                    json_data=analysis_data, iterations=10)
        
        print("=" * 60)
        print("üìä Benchmark Summary")
        print("=" * 60)
        
        if self.results:
            # Sort by average response time
            sorted_results = sorted(self.results, key=lambda x: x['avg_time'])
            
            print("üèÜ Fastest to Slowest Endpoints:")
            for i, result in enumerate(sorted_results, 1):
                print(f"{i}. {result['method']} {result['endpoint']}: "
                      f"{result['avg_time']*1000:.2f}ms avg, "
                      f"{result['success_rate']:.1f}% success")
            
            # Generate CSV report
            self.generate_csv_report()
        
        return self.results
    
    async def stress_test(self, concurrent_users: int = 10, duration_seconds: int = 30):
        """Stress test with concurrent users"""
        print(f"üî• Stress Test: {concurrent_users} concurrent users for {duration_seconds}s")
        print("=" * 60)
        
        async def user_simulation(user_id: int):
            """Simulate a user making requests"""
            requests_made = 0
            errors = 0
            start_time = time.time()
            
            async with httpx.AsyncClient(base_url=self.base_url, timeout=10.0) as client:
                while time.time() - start_time < duration_seconds:
                    try:
                        # Simulate typical user behavior
                        endpoints = [
                            "/health",
                            "/api/v1/dashboard/stats",
                            "/api/v1/dashboard/trends?days=7",
                        ]
                        
                        endpoint = endpoints[requests_made % len(endpoints)]
                        response = await client.get(endpoint, headers=self.headers)
                        
                        if response.status_code == 200:
                            requests_made += 1
                        else:
                            errors += 1
                        
                        # Simulate user think time
                        await asyncio.sleep(0.5)
                        
                    except Exception:
                        errors += 1
                    
            return {
                "user_id": user_id,
                "requests": requests_made,
                "errors": errors,
                "duration": time.time() - start_time
            }
        
        # Run concurrent users
        tasks = [user_simulation(i) for i in range(concurrent_users)]
        user_results = await asyncio.gather(*tasks)
        
        # Calculate aggregate stats
        total_requests = sum(r["requests"] for r in user_results)
        total_errors = sum(r["errors"] for r in user_results)
        avg_requests_per_user = total_requests / concurrent_users
        error_rate = (total_errors / (total_requests + total_errors)) * 100 if (total_requests + total_errors) > 0 else 0
        
        print(f"üìä Stress Test Results:")
        print(f"   Total Requests: {total_requests}")
        print(f"   Total Errors: {total_errors}")
        print(f"   Error Rate: {error_rate:.2f}%")
        print(f"   Avg Requests/User: {avg_requests_per_user:.1f}")
        print(f"   Requests/Second: {total_requests / duration_seconds:.2f}")
        
        return user_results
    
    def generate_csv_report(self):
        """Generate CSV report of benchmark results"""
        filename = f"benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        with open(filename, 'w', newline='') as csvfile:
            fieldnames = [
                'endpoint', 'method', 'iterations', 'success_rate',
                'avg_time_ms', 'median_time_ms', 'p95_time_ms', 'p99_time_ms',
                'requests_per_second', 'errors'
            ]
            
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for result in self.results:
                row = {
                    'endpoint': result['endpoint'],
                    'method': result['method'],
                    'iterations': result['iterations'],
                    'success_rate': f"{result['success_rate']:.2f}",
                    'avg_time_ms': f"{result['avg_time']*1000:.2f}",
                    'median_time_ms': f"{result['median_time']*1000:.2f}",
                    'p95_time_ms': f"{result['p95_time']*1000:.2f}",
                    'p99_time_ms': f"{result['p99_time']*1000:.2f}",
                    'requests_per_second': f"{result['requests_per_second']:.2f}",
                    'errors': result['errors']
                }
                writer.writerow(row)
        
        print(f"üìä Benchmark report saved to: {filename}")


async def main():
    """Main benchmarking function"""
    benchmark = PerformanceBenchmark()
    
    print("üî¨ Honeypot Detector - Performance Benchmark Suite")
    print("Make sure the API is running on http://localhost:8000")
    print("=" * 60)
    
    # Run comprehensive benchmark
    await benchmark.run_comprehensive_benchmark()
    
    # Run stress test
    await benchmark.stress_test(concurrent_users=5, duration_seconds=15)
    
    print("\nüéâ Performance benchmarking completed!")
    print("Check the generated CSV file for detailed results.")


if __name__ == "__main__":
    asyncio.run(main())
