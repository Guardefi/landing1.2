#!/usr/bin/env python3
"""
Clean Test Runner for Scorpius Enterprise Platform
Handles various test formats and dependency issues gracefully.
"""

import asyncio
import os
import sys
import traceback
from pathlib import Path
import subprocess
import json
import time

# Add parent directories to path for imports
project_root = Path(__file__).parent
while project_root.name != "Scorpius-main" and project_root != project_root.parent:
    project_root = project_root.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "backend"))
sys.path.insert(0, str(project_root / "packages" / "core"))

class MockSimilarityEngine:
    def __init__(self, *args, **kwargs): 
        pass
    
    async def compare_bytecodes(self, *args, **kwargs): 
        class Result:
            similarity_score = 0.85
            confidence = 0.9
            processing_time = 0.01
        return Result()
    
    async def cleanup(self): 
        pass

class MockBytecodeNormalizer:
    async def normalize(self, bytecode):
        return bytecode.replace("0x", "").lower() if bytecode else ""

class MockMultiDimensionalComparison:
    def __init__(self, *args, **kwargs): 
        pass
    
    async def compute_similarity(self, b1, b2):
        return {"final_score": 0.85, "confidence": 0.9, "dimension_scores": {}}

class MockTestClient:
    def __init__(self, app): 
        self.app = app
    
    def get(self, url): 
        class Response:
            status_code = 200
            def json(self): 
                return {"status": "ok"}
        return Response()

# Add mocks to globals for import fallbacks
globals().update({
    'SimilarityEngine': MockSimilarityEngine,
    'BytecodeNormalizer': MockBytecodeNormalizer, 
    'MultiDimensionalComparison': MockMultiDimensionalComparison,
    'TestClient': MockTestClient,
})

class ScorpiusTestRunner:
    def __init__(self):
        self.results = {
            "total_found": 0,
            "total_run": 0,
            "passed": 0,
            "failed": 0,
            "skipped": 0,
            "errors": 0,
            "details": []
        }
        self.project_root = Path(__file__).parent
        
    def find_test_files(self):
        """Find all test files in the project"""
        test_files = []
        for pattern in ["test_*.py", "*_test.py"]:
            test_files.extend(list(self.project_root.rglob(pattern)))
        return test_files
    
    def run_python_file_directly(self, test_file):
        """Run a Python test file directly"""
        try:
            original_cwd = os.getcwd()
            test_dir = test_file.parent
            os.chdir(test_dir)
            
            sys.path.insert(0, str(test_dir))
            
            result = subprocess.run([
                sys.executable, str(test_file)
            ], capture_output=True, text=True, timeout=30, encoding='utf-8', errors='replace')
            
            os.chdir(original_cwd)
            
            return {
                "success": result.returncode == 0,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "returncode": result.returncode
            }
            
        except subprocess.TimeoutExpired:
            return {
                "success": False,
                "stdout": "",
                "stderr": "Test timed out after 30 seconds",
                "returncode": -1
            }
        except Exception as e:
            return {
                "success": False,
                "stdout": "",
                "stderr": f"Error running test: {str(e)}",
                "returncode": -1
            }
        finally:
            os.chdir(original_cwd)
    
    def analyze_test_file(self, test_file):
        """Analyze a test file to understand its format"""
        try:
            with open(test_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            analysis = {
                "has_pytest": "pytest" in content,
                "has_unittest": "unittest" in content or "TestCase" in content,
                "has_asyncio": "asyncio" in content or "async def" in content,
                "has_main": '__name__ == "__main__"' in content,
                "imports": []
            }
            
            lines = content.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('import ') or line.startswith('from '):
                    analysis["imports"].append(line)
            
            return analysis
            
        except Exception as e:
            return {"error": str(e)}
    
    def run_single_test(self, test_file):
        """Run a single test file"""
        print(f"\n{'='*60}")
        print(f"Running: {test_file.relative_to(self.project_root)}")
        print(f"{'='*60}")
        
        analysis = self.analyze_test_file(test_file)
        
        if "error" in analysis:
            print(f"[FAIL] Error analyzing test file: {analysis['error']}")
            self.results["errors"] += 1
            self.results["details"].append({
                "file": str(test_file.relative_to(self.project_root)),
                "status": "error",
                "message": analysis["error"]
            })
            return
        
        print(f"Analysis: pytest={analysis['has_pytest']}, unittest={analysis['has_unittest']}, asyncio={analysis['has_asyncio']}")
        
        if analysis.get("has_main"):
            result = self.run_python_file_directly(test_file)
            
            if result["success"]:
                print("[PASS] PASSED")
                self.results["passed"] += 1
                self.results["details"].append({
                    "file": str(test_file.relative_to(self.project_root)),
                    "status": "passed",
                    "message": "Test completed successfully"
                })
            else:
                print(f"[FAIL] FAILED")
                print(f"Return code: {result['returncode']}")
                if result["stdout"]:
                    print(f"STDOUT:\n{result['stdout']}")
                if result["stderr"]:
                    print(f"STDERR:\n{result['stderr']}")
                
                self.results["failed"] += 1
                self.results["details"].append({
                    "file": str(test_file.relative_to(self.project_root)),
                    "status": "failed",
                    "message": f"Exit code {result['returncode']}: {result['stderr']}"
                })
        else:
            print("[SKIP] SKIPPED (No main block)")
            self.results["skipped"] += 1
            self.results["details"].append({
                "file": str(test_file.relative_to(self.project_root)),
                "status": "skipped",
                "message": "No main execution block found"
            })
        
        self.results["total_run"] += 1
    
    def run_all_tests(self):
        """Run all tests in the project"""
        print(">> Discovering test files...")
        test_files = self.find_test_files()
        
        print(f"Found {len(test_files)} test files")
        self.results["total_found"] = len(test_files)
        
        problematic_patterns = [
            "conftest.py",
            "__pycache__",
            "fix_",
            "test_runner"
        ]
        
        filtered_files = []
        for test_file in test_files:
            skip = False
            for pattern in problematic_patterns:
                if pattern in str(test_file):
                    skip = True
                    break
            if not skip:
                filtered_files.append(test_file)
        
        print(f"Running {len(filtered_files)} test files (filtered out {len(test_files) - len(filtered_files)} problematic files)")
        
        for test_file in filtered_files:
            try:
                self.run_single_test(test_file)
            except KeyboardInterrupt:
                print("\n>> Test run interrupted by user")
                break
            except Exception as e:
                print(f"[FAIL] Unexpected error running {test_file}: {e}")
                self.results["errors"] += 1
                self.results["details"].append({
                    "file": str(test_file.relative_to(self.project_root)),
                    "status": "error",
                    "message": f"Unexpected error: {str(e)}"
                })
                self.results["total_run"] += 1
    
    def print_summary(self):
        """Print test results summary"""
        print(f"\n{'='*60}")
        print("TEST SUMMARY")
        print(f"{'='*60}")
        print(f"Total found: {self.results['total_found']}")
        print(f"Total run: {self.results['total_run']}")
        print(f"Passed: {self.results['passed']}")
        print(f"Failed: {self.results['failed']}")
        print(f"Skipped: {self.results['skipped']}")
        print(f"Errors: {self.results['errors']}")
        
        if self.results['total_run'] > 0:
            success_rate = (self.results['passed'] / self.results['total_run']) * 100
            print(f"Success rate: {success_rate:.1f}%")
        
        with open("test_results.json", "w") as f:
            json.dump(self.results, f, indent=2)
        
        print(f"\nDetailed results saved to test_results.json")

def main():
    """Main function to run all tests"""
    print(">> Scorpius Enterprise Test Runner")
    print("=" * 50)
    
    runner = ScorpiusTestRunner()
    
    try:
        runner.run_all_tests()
    finally:
        runner.print_summary()
    
    return runner.results['passed'] > 0

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 